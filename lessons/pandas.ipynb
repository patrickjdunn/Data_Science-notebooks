{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28a2f9ce",
   "metadata": {},
   "source": [
    "## üìÅ Understanding the Project Structure\n",
    "\n",
    "This notebook is part of a larger project that contains **many datasets** stored in a shared `data/` folder.\n",
    "\n",
    "Because this notebook lives in the `lessons/` folder, we cannot assume that the current working directory is the project root. Instead, we programmatically locate the project root and then build paths from there.\n",
    "\n",
    "This approach prevents common file-not-found errors and works even if notebooks are moved to different folders.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## üóÇÔ∏è Step 4: View a Catalog of Available Datasets\n",
    "\n",
    "After loading the data, we create a summary table that lists:\n",
    "\n",
    "* Dataset name\n",
    "* Number of rows\n",
    "* Number of columns\n",
    "\n",
    "This table functions as a lightweight **data catalog** and helps you quickly understand what data is available for analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## üëÄ Step 5: Preview Individual Datasets\n",
    "\n",
    "To explore a dataset, we can display:\n",
    "\n",
    "* The first few rows\n",
    "* Column names\n",
    "* Dataset dimensions\n",
    "* Missing values\n",
    "\n",
    "This is an important step in **exploratory data analysis (EDA)** and helps you understand:\n",
    "\n",
    "* What each variable represents\n",
    "* Which variables may require cleaning or transformation\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Step 6: Understanding Missing Values\n",
    "\n",
    "Some datasets intentionally contain missing values.\n",
    "\n",
    "These are not errors ‚Äî they represent real-world data challenges that analysts must address.\n",
    "\n",
    "In this course, you will:\n",
    "\n",
    "* Identify missing values\n",
    "* Decide how to handle them (drop, impute, or analyze separately)\n",
    "* Learn how missing data affects analysis and modeling\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Why This Workflow Matters\n",
    "\n",
    "This notebook demonstrates best practices used by professional data scientists:\n",
    "\n",
    "* Writing **reusable code**\n",
    "* Avoiding hard-coded file paths\n",
    "* Verifying data before analysis\n",
    "* Separating data loading from modeling\n",
    "\n",
    "Mastering this workflow will make your future analyses:\n",
    "\n",
    "* More reliable\n",
    "* Easier to debug\n",
    "* Easier to scale to larger projects\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ What Comes Next\n",
    "\n",
    "With all datasets loaded, you are now ready to:\n",
    "\n",
    "* Explore relationships between variables\n",
    "* Join datasets together\n",
    "* Clean and transform data\n",
    "* Build statistical or machine learning models\n",
    "\n",
    "Each future lesson will build on the foundation established here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcf69d4",
   "metadata": {},
   "source": [
    "## üìç Step 1: Locate the Data Directory\n",
    "\n",
    "In this step, we:\n",
    "\n",
    "* Identify the project‚Äôs root directory\n",
    "* Build a reliable path to the `data/` folder\n",
    "* Verify that the folder exists\n",
    "\n",
    "```python\n",
    "REPO_ROOT = Path.cwd().parents[0]\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "```\n",
    "\n",
    "If the data directory exists, we know the notebook is correctly configured.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0089d59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATA_DIR = Path.cwd().parent / \"data\"\n",
    "\n",
    "print(\"Working directory:\", Path.cwd())\n",
    "print(\"Data directory:\", DATA_DIR)\n",
    "print(\"Data directory exists:\", DATA_DIR.exists())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ceeca8d",
   "metadata": {},
   "source": [
    "## üì¶ Step 2: Load All CSV Files Automatically\n",
    "\n",
    "Rather than loading each dataset manually, we scan the `data/` folder and load **every CSV file** into pandas.\n",
    "\n",
    "Each dataset is stored in a dictionary called `dfs`, where:\n",
    "\n",
    "* The **key** is the dataset name\n",
    "* The **value** is a pandas DataFrame\n",
    "\n",
    "This allows us to work with many datasets using a consistent and scalable approach.\n",
    "\n",
    "Missing values such as `\"NA\"` or empty cells are automatically converted to `NaN`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5d9e95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded atbats                         shape=(528, 23)\n",
      "Loaded batter_player                  shape=(2105, 11)\n",
      "Loaded bp_readings                    shape=(9, 7)\n",
      "Loaded cardio                         shape=(70000, 30)\n",
      "Loaded cardio1                        shape=(70000, 24)\n",
      "Loaded cardio10                       shape=(67066, 27)\n",
      "Loaded game_player                    shape=(2105, 11)\n",
      "Loaded games                          shape=(100, 18)\n",
      "Loaded medications                    shape=(5, 7)\n",
      "Loaded patients                       shape=(5, 8)\n",
      "Loaded pitches                        shape=(451, 32)\n",
      "Loaded pitches_w_inplay               shape=(29616, 33)\n",
      "Loaded pitches_w_inplay_nb            shape=(29616, 33)\n",
      "Loaded players                        shape=(825, 13)\n",
      "Loaded players100                     shape=(911, 5)\n",
      "Loaded players_cuba                   shape=(17, 6)\n",
      "Loaded players_cuba_no_index          shape=(17, 5)\n",
      "Loaded signatures_simulated_dataset   shape=(1001, 28)\n",
      "Loaded teams                          shape=(30, 43)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "REPO_ROOT = Path.cwd().parents[0]\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "\n",
    "csv_files = sorted(DATA_DIR.glob(\"*.csv\"))\n",
    "\n",
    "dfs = {}\n",
    "\n",
    "for f in csv_files:\n",
    "    name = f.stem.replace(\"-\", \"_\")\n",
    "    dfs[name] = pd.read_csv(\n",
    "        f,\n",
    "        na_values=[\"NA\", \"\"],\n",
    "        keep_default_na=True\n",
    "    )\n",
    "    print(f\"Loaded {name:30s} shape={dfs[name].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a55f5d",
   "metadata": {},
   "source": [
    "## ‚úÖ Step 3: Confirm Successful Data Loading\n",
    "\n",
    "As each file is loaded, the notebook prints:\n",
    "\n",
    "* The dataset name\n",
    "* The number of rows\n",
    "* The number of columns\n",
    "\n",
    "Example output:\n",
    "\n",
    "```\n",
    "Loaded cardio10 shape=(67066, 27)\n",
    "```\n",
    "\n",
    "This acts as a **sanity check** to confirm that all files loaded correctly.\n",
    "\n",
    "If a dataset is missing or malformed, it will be immediately obvious.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de9c719",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfc5022a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
